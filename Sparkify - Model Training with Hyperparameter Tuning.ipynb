{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace - Model Training with Hyperparameter Tuning\n",
    "\n",
    "This notebook explores how different customer characteristics affect the probability of churning from a music streaming subscription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for a Spark session to start...\n",
      "Spark Initialization Done! ApplicationId = app-20210104234026-0000\n",
      "KERNEL_ID = 0fe4cde4-91a1-4495-ab89-eb9d80d98783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import gc\n",
    "import pyspark\n",
    "from pyspark.sql.types import IntegerType, StringType, DoubleType, LongType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import  desc, explode, lit, split, udf, count, avg, col, concat\\\n",
    ",mean,when,lead,isnan,countDistinct,month,from_unixtime,datediff,to_timestamp\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.context \n",
    "from pyspark import SparkConf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression,RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import MinMaxScaler, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ibmos2spark\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3.ap-geo.objectstorage.service.networklayer.com',\n",
    "    'service_id': 'iam-ServiceId-**REMOVED**',\n",
    "    'iam_service_endpoint': 'https://iam.cloud.ibm.com/oidc/token',\n",
    "    'api_key': 'A1hDyHu7cqIIXIKH4femE9WnyBlukXMkoBeWfb4DZCqm'\n",
    "}\n",
    "\n",
    "configuration_name = 'os_878688b9464f4f5e93f45a4ed3683f7f_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Load and Clean Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Loading the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(cos.url('medium-sparkify-event-data.json', 'sparkifychurnprediction-donotdelete-pr-gh0yoxzekm7cww'))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the rows with blank User IDs\n",
    "df = df.filter(df['userId']!='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final User ID count\n",
    "df.select('userId').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4470"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final Session ID count\n",
    "df.select('sessionId').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the UNIX millisecond timestamp to datetime\n",
    "to_date = udf(lambda x: datetime.datetime.fromtimestamp(x/1000.0) \\\n",
    "             .strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "df = df.withColumn('ts_date', to_date('ts'))\n",
    "df = df.withColumn('ts_registration', to_date('registration'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Exploratory Data Analysis\n",
    "This part explores the dataset for patterns before jumping into feature engineering and modeling.\n",
    "\n",
    "## 2.1 Define Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users who cancelled subscription\n",
    "churn_event = udf(lambda x: 1 if x == 'Cancellation Confirmation' else 0, IntegerType())\n",
    "df = df.withColumn('churn_event', churn_event('page'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users who downgraded\n",
    "downgrade_event = udf(lambda x: 1 if x == 'Downgrade' else 0, IntegerType())\n",
    "df = df.withColumn('downgrade_event', downgrade_event('page'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a user-level dataset with userId and churn / downgrade markers\n",
    "df_user = df.groupBy('userId').agg({'churn_event':'max', 'downgrade_event':'max'}) \\\n",
    "            .withColumnRenamed('max(churn_event)', 'churn') \\\n",
    "            .withColumnRenamed('max(downgrade_event)', 'downgrade') \n",
    "\n",
    "df_user.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Initial Data Exploration\n",
    "Join some existing fields into the user dataset, and add some features that we can explore by churn status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[userId: string, gender: string, location: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('userId','gender','location').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join Gender & Location data to user data\n",
    "df_demogs = df.select('userId','gender','location','level').distinct()\n",
    "df_user = df_user.join(df_demogs, on='userId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Feature Engineering\n",
    "Some features that can possibly affect churn rates can be derived from the dataset. Here are some of the features that I think will be good predictors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate Features:\n",
    "\n",
    "1. Total sessions in a week\n",
    "2. Total number of songs listened to\n",
    "3. Total number of artists listened to\n",
    "4. Number of thumbs up\n",
    "5. Number of thumbs down\n",
    "6. Number of friend requests sent\n",
    "7. Number of add to playlist\n",
    "8. Number of songs played in a session\n",
    "9. Average session length\n",
    "10. Average Gap (number of days) in between visit\n",
    "11. Days from registration date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of songs listened to\n",
    "song_count = df.groupBy('userId').agg(countDistinct('song')).withColumnRenamed('count(DISTINCT song)','song_count')\n",
    "# Number of artists listened to\n",
    "artist_count = df.groupBy('userId').agg(countDistinct('artist')).withColumnRenamed('count(DISTINCT artist)','artist_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count number of instances depending on the page action\n",
    "def count_page(df, condition):\n",
    "    filter_str = 'page == \"' + condition + '\"'\n",
    "    return df.filter(filter_str).groupBy('userId').count()\n",
    "    \n",
    "page_thumbsup = count_page(df, 'Thumbs Up').withColumnRenamed('count','thumbsup_count')\n",
    "page_thumbsdown = count_page(df, 'Thumbs Down').withColumnRenamed('count','thumbsdown_count')\n",
    "page_addfriend = count_page(df, 'Add Friend').withColumnRenamed('count','addfriend_count')\n",
    "page_addplaylist = count_page(df, 'Add to Playlist').withColumnRenamed('count','addplaylist_count')\n",
    "page_error = count_page(df, 'Error').withColumnRenamed('count','pageerror_count')\n",
    "page_upgrade = count_page(df, 'Upgrade').withColumnRenamed('count','upgrade_count')\n",
    "page_nextsong = count_page(df, 'NextSong').withColumnRenamed('count','nextsong_count')\n",
    "page_help = count_page(df, 'Help').withColumnRenamed('count','help_count')\n",
    "page_rolladvert = count_page(df, 'Roll Advert').withColumnRenamed('count','rolladvert_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Session Length\n",
    "avg_session_length = df.groupBy('userId').mean('length').withColumnRenamed('avg(length)','avg_session_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total sessions in a week\n",
    "session_wk = df.groupBy('userId').agg(F.min('ts_date'), F.max('ts_date'), F.countDistinct('sessionId')) \\\n",
    "            .withColumnRenamed('min(ts_date)', 'min_ts_date') \\\n",
    "            .withColumnRenamed('max(ts_date)', 'max_ts_date') \\\n",
    "            .withColumnRenamed('count(DISTINCT sessionId)', 'total_sessions')\n",
    "\n",
    "def calc_sessions_wk(session_wk, max_date, min_date, all_sessions):\n",
    "    return F.when(datediff(session_wk[max_date], session_wk[min_date]) == 0, session_wk[all_sessions] / 7) \\\n",
    "    .otherwise((session_wk[all_sessions] / datediff(session_wk[max_date], session_wk[min_date])) * 7)\n",
    "\n",
    "session_wk = session_wk.withColumn('avg_sessions_wk', calc_sessions_wk(session_wk, 'max_ts_date', 'min_ts_date', 'total_sessions'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate visit gap\n",
    "\n",
    "# For each user, get all the dates that they had any session (ts_date)\n",
    "user_by_session = df.select('userId', 'ts_date').distinct().sort(['userId', 'ts_date'])\n",
    "# Use a window function to shift the dates and get the dates for the next session\n",
    "user_by_session = user_by_session.withColumn('date_next_session',lead('ts_date',1)\\\n",
    "                .over(Window.partitionBy('userId').orderBy('ts_date'))) \\\n",
    "                .filter(F.col('date_next_session').isNotNull())\n",
    "\n",
    "# Get the average gap per user\n",
    "user_by_session = user_by_session.withColumn('gap', datediff(F.col('date_next_session'), F.col('ts_date')))\n",
    "user_by_session = user_by_session.groupBy('userId').mean('gap').withColumnRenamed('avg(gap)', 'avg_session_gap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Days from registration date\n",
    "\n",
    "# For each user, get the max ts_date\n",
    "user_by_registration = df.select('userId','ts_registration', 'ts_date').distinct().sort('userId',desc('ts_date'))\n",
    "user_by_registration = user_by_registration.toPandas()\n",
    "user_by_registration = user_by_registration.drop_duplicates(subset=['userId','ts_registration'], keep='first', inplace=False)\n",
    "user_by_registration = spark.createDataFrame(user_by_registration)\n",
    "user_by_registration = user_by_registration.withColumn('days_from_reg', datediff('ts_date','ts_registration'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert gender and level to int to be used for modeling\n",
    "\n",
    "# Gender to int\n",
    "int_gender = udf(lambda x: 1 if x == 'F' else 0, IntegerType())\n",
    "df_user = df_user.withColumn('int_gender',int_gender('gender'))\n",
    "gender_int = df_user.groupBy('userId').agg(F.max('int_gender')).withColumnRenamed('max(int_gender)','gender')\n",
    "\n",
    "# Level to int\n",
    "int_level = udf(lambda x: 1 if x == 'paid' else 0, IntegerType())\n",
    "df = df.withColumn('int_level',int_level('level'))\n",
    "level_int = df.groupBy('userId').agg(F.max('int_level')).withColumnRenamed('max(int_level)','level')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging everything into one Spark dataframe for modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new user_df only including the userId and all relevant features for modeling\n",
    "df_user = df_user.select(['userId','churn','downgrade']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all the features\n",
    "df_user = df_user.join(gender_int, 'userId') \\\n",
    ".join(level_int, 'userId') \\\n",
    ".join(song_count, 'userId') \\\n",
    ".join(artist_count, 'userId') \\\n",
    ".join(page_thumbsup, 'userId') \\\n",
    ".join(page_thumbsdown, 'userId') \\\n",
    ".join(page_addfriend, 'userId') \\\n",
    ".join(page_addplaylist, 'userId') \\\n",
    ".join(page_error, 'userId') \\\n",
    ".join(page_upgrade, 'userId') \\\n",
    ".join(page_nextsong, 'userId') \\\n",
    ".join(page_help, 'userId') \\\n",
    ".join(page_rolladvert, 'userId') \\\n",
    ".join(avg_session_length, 'userId') \\\n",
    ".join(session_wk, 'userId') \\\n",
    ".join(user_by_session, 'userId') \\\n",
    ".join(user_by_registration, 'userId') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop some of the variables we won't be needing for modeling\n",
    "columns_to_drop = ['min_ts_date', 'max_ts_date', 'ts_registration', 'ts_date']\n",
    "df_user = df_user.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- churn: integer (nullable = true)\n",
      " |-- downgrade: integer (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      " |-- level: integer (nullable = true)\n",
      " |-- song_count: long (nullable = false)\n",
      " |-- artist_count: long (nullable = false)\n",
      " |-- thumbsup_count: long (nullable = false)\n",
      " |-- thumbsdown_count: long (nullable = false)\n",
      " |-- addfriend_count: long (nullable = false)\n",
      " |-- addplaylist_count: long (nullable = false)\n",
      " |-- pageerror_count: long (nullable = false)\n",
      " |-- upgrade_count: long (nullable = false)\n",
      " |-- nextsong_count: long (nullable = false)\n",
      " |-- help_count: long (nullable = false)\n",
      " |-- rolladvert_count: long (nullable = false)\n",
      " |-- avg_session_length: double (nullable = true)\n",
      " |-- total_sessions: long (nullable = false)\n",
      " |-- avg_sessions_wk: double (nullable = true)\n",
      " |-- avg_session_gap: double (nullable = true)\n",
      " |-- days_from_reg: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_user.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_model = df_user.drop('userId')\n",
    "features = [col for col in df_user_model.columns if col != 'churn'] \n",
    "train, test = df_user_model.randomSplit([0.7, 0.3], seed=13)\n",
    "train = train.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(classifier, params):\n",
    "    \"\"\"\n",
    "    Function to build a model pipeline \n",
    "    Input:\n",
    "    classifier: type of classifier\n",
    "    params: param grid\n",
    "    Output:\n",
    "    ML pipeline model\n",
    "    \"\"\"\n",
    "    assembler = VectorAssembler(inputCols=features, outputCol='features')\n",
    "    scaler = MinMaxScaler(inputCol='features', outputCol='scaled_features')\n",
    "    pipeline = Pipeline(stages=[assembler, scaler, classifier])\n",
    "\n",
    "    model = CrossValidator(\n",
    "        estimator=pipeline,\n",
    "        estimatorParamMaps=params,\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol='churn', metricName='f1'),\n",
    "        numFolds=3,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LogisticRegression(featuresCol=\"scaled_features\", labelCol=\"churn\")\n",
    "model1_param = ParamGridBuilder().build()\n",
    "model_lr = model_pipeline(model1, model1_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model_lr = model_lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lr = fit_model_lr.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score using Logistic Regression: 0.8706151357983382\n",
      "Precision using Logistic Regression: 0.8694559800664451\n",
      "Recall using Logistic Regression: 0.872093023255814\n",
      "Accuracy using Logistic Regression: 0.872093023255814\n"
     ]
    }
   ],
   "source": [
    "pred_lr.select(\"prediction\").dropDuplicates().collect()\n",
    "evaluator_lr = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"churn\")\n",
    "\n",
    "f1_score_lr = evaluator_lr.evaluate(pred_lr, {evaluator_lr.metricName: \"f1\"})\n",
    "precision_lr = evaluator_lr.evaluate(pred_lr, {evaluator_lr.metricName: \"weightedPrecision\"})\n",
    "recall_lr = evaluator_lr.evaluate(pred_lr, {evaluator_lr.metricName: \"weightedRecall\"})\n",
    "accuracy_lr = evaluator_lr.evaluate(pred_lr, {evaluator_lr.metricName: \"accuracy\"})\n",
    "\n",
    "print(\"F1-score using Logistic Regression: {}\".format(f1_score_lr))\n",
    "print(\"Precision using Logistic Regression: {}\".format(precision_lr))\n",
    "print(\"Recall using Logistic Regression: {}\".format(recall_lr))\n",
    "print(\"Accuracy using Logistic Regression: {}\".format(accuracy_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = RandomForestClassifier(featuresCol=\"scaled_features\", labelCol=\"churn\", seed=13)\n",
    "model2_param = ParamGridBuilder().build()\n",
    "model_rf = model_pipeline(model2, model2_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model_rf = model_rf.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rf = fit_model_rf.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score using Random Forest: 0.9100181175562089\n",
      "Precision using Random Forest: 0.9261015911872705\n",
      "Recall using Random Forest: 0.9186046511627907\n",
      "Accuracy using Random Forest: 0.9186046511627907\n"
     ]
    }
   ],
   "source": [
    "pred_rf.select(\"prediction\").dropDuplicates().collect()\n",
    "evaluator_rf = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"churn\")\n",
    "\n",
    "f1_score_rf = evaluator_rf.evaluate(pred_rf, {evaluator_rf.metricName: \"f1\"})\n",
    "precision_rf = evaluator_rf.evaluate(pred_rf, {evaluator_rf.metricName: \"weightedPrecision\"})\n",
    "recall_rf = evaluator_rf.evaluate(pred_rf, {evaluator_rf.metricName: \"weightedRecall\"})\n",
    "accuracy_rf = evaluator_rf.evaluate(pred_rf, {evaluator_rf.metricName: \"accuracy\"})\n",
    "\n",
    "print(\"F1-score using Random Forest: {}\".format(f1_score_rf))\n",
    "print(\"Precision using Random Forest: {}\".format(precision_rf))\n",
    "print(\"Recall using Random Forest: {}\".format(recall_rf))\n",
    "print(\"Accuracy using Random Forest: {}\".format(accuracy_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Gradient Boosting Trees (GBT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = GBTClassifier(featuresCol=\"scaled_features\", labelCol=\"churn\", seed=13)\n",
    "model3_param = ParamGridBuilder().build()\n",
    "model_gbt = model_pipeline(model3, model3_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model_gbt = model_gbt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_gbt = fit_model_gbt.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score using Gradient Boosting Trees: 0.9637994605034236\n",
      "Precision using Gradient Boosting Trees: 0.9665697674418605\n",
      "Recall using Gradient Boosting Trees: 0.9651162790697674\n",
      "Accuracy using Gradient Boosting Trees: 0.9651162790697675\n"
     ]
    }
   ],
   "source": [
    "pred_gbt.select(\"prediction\").dropDuplicates().collect()\n",
    "evaluator_gbt = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"churn\")\n",
    "\n",
    "f1_score_gbt = evaluator_gbt.evaluate(pred_gbt, {evaluator_gbt.metricName: \"f1\"})\n",
    "precision_gbt = evaluator_gbt.evaluate(pred_gbt, {evaluator_gbt.metricName: \"weightedPrecision\"})\n",
    "recall_gbt = evaluator_gbt.evaluate(pred_gbt, {evaluator_gbt.metricName: \"weightedRecall\"})\n",
    "accuracy_gbt = evaluator_gbt.evaluate(pred_gbt, {evaluator_gbt.metricName: \"accuracy\"})\n",
    "\n",
    "print(\"F1-score using Gradient Boosting Trees: {}\".format(f1_score_gbt))\n",
    "print(\"Precision using Gradient Boosting Trees: {}\".format(precision_gbt))\n",
    "print(\"Recall using Gradient Boosting Trees: {}\".format(recall_gbt))\n",
    "print(\"Accuracy using Gradient Boosting Trees: {}\".format(accuracy_gbt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(featuresCol=\"scaled_features\", labelCol=\"churn\", seed=13)\n",
    "paramGrid_gbt = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxBins, [32, 50]) \\\n",
    "    .addGrid(gbt.maxIter, [20, 30]) \\\n",
    "    .build()\n",
    "model_tuned = model_pipeline(gbt, paramGrid_gbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model_tuned = model_tuned.fit(train)                                       \n",
    "best_model = fit_model_tuned.bestModel\n",
    "best_model_pred = best_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score using Gradient Boosting Trees (Best Model): 0.9534883720930233\n",
      "Precision using Gradient Boosting Trees (Best Model): 0.9534883720930233\n",
      "Recall using Gradient Boosting Trees (Best Model): 0.9534883720930233\n",
      "Accuracy using Gradient Boosting Trees (Best Model): 0.9534883720930233\n"
     ]
    }
   ],
   "source": [
    "best_model_pred.select(\"prediction\").dropDuplicates().collect()\n",
    "evaluator_best_model = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"churn\")\n",
    "\n",
    "f1_score_best_model = evaluator_best_model.evaluate(best_model_pred, {evaluator_best_model.metricName: \"f1\"})\n",
    "precision_best_model = evaluator_best_model.evaluate(best_model_pred, {evaluator_best_model.metricName: \"weightedPrecision\"})\n",
    "recall_best_model = evaluator_best_model.evaluate(best_model_pred, {evaluator_best_model.metricName: \"weightedRecall\"})\n",
    "accuracy_best_model = evaluator_best_model.evaluate(best_model_pred, {evaluator_best_model.metricName: \"accuracy\"})\n",
    "\n",
    "print(\"F1-score using Gradient Boosting Trees (Best Model): {}\".format(f1_score_best_model))\n",
    "print(\"Precision using Gradient Boosting Trees (Best Model): {}\".format(precision_best_model))\n",
    "print(\"Recall using Gradient Boosting Trees (Best Model): {}\".format(recall_best_model))\n",
    "print(\"Accuracy using Gradient Boosting Trees (Best Model): {}\".format(accuracy_best_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxIter for the best model: 30\n",
      "MaxBins for the best model: 50\n"
     ]
    }
   ],
   "source": [
    "print(\"MaxIter for the best model: {}\".format(fit_model_tuned.bestModel.stages[2]._java_obj.getMaxIter()))\n",
    "print(\"MaxBins for the best model: {}\".format(fit_model_tuned.bestModel.stages[2]._java_obj.getMaxBins()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxIter for the original model: 20\n",
      "MaxBins for the original model: 32\n"
     ]
    }
   ],
   "source": [
    "# For reference, hyperparameters of original, untuned model:\n",
    "print(\"MaxIter for the original model: {}\".format(fit_model_gbt.bestModel.stages[2]._java_obj.getMaxIter()))\n",
    "print(\"MaxBins for the original model: {}\".format(fit_model_gbt.bestModel.stages[2]._java_obj.getMaxBins()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
